<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns#">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <title>Estimating Mutual Information in Bayesian Neural Networks &middot; David Blitz</title>
        <meta name="description" content="Let&rsquo;s consider a Bayesian Neural Network, where we consider the hidden layers as random variables, \(Z_i\). We also consider the input, \(X\), and the learning task, \(Y\), as random variables. Our goal is to estimate the mutual information between the input and the layers - \(I(X, Z_i)\) - as well as between the layers and the task - \(I(Z_i, Y)\). For this, we will make use of the probabilistic forward propagation approximation, as introduced [in the PBP paper].">
        <meta name="HandheldFriendly" content="True">
        <meta name="MobileOptimized" content="320">
        <meta name="generator" content="Hugo 0.101.0" />
        <meta name="robots" content="index,follow">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta property="og:title" content="Estimating Mutual Information in Bayesian Neural Networks">
<meta property="og:description" content="Let&rsquo;s consider a Bayesian Neural Network, where we consider the hidden layers as random variables, \(Z_i\). We also consider the input, \(X\), and the learning task, \(Y\), as random variables. Our goal is to estimate the mutual information between the input and the layers - \(I(X, Z_i)\) - as well as between the layers and the task - \(I(Z_i, Y)\). For this, we will make use of the probabilistic forward propagation approximation, as introduced [in the PBP paper].">
<meta property="og:type" content="article">
<meta property="og:url" content="https://davidblitz.github.io/2019/02/18/estimating-mutual-information-in-bnn/">
        <link rel="stylesheet" href="https://davidblitz.github.io/dist/site.css">
        <link rel="stylesheet" href="https://davidblitz.github.io/dist/syntax.css">
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,400,600,700,300&subset=latin,cyrillic-ext,latin-ext,cyrillic">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
        
        
        
        
        

    </head>
    <body>
        

        <div id="wrapper">
            <header class="site-header">
                <div class="container">
                    <div class="site-title-wrapper">
                        
                            <h1 class="site-title">
                                <a href="https://davidblitz.github.io/">BlitzBlog</a>
                            </h1>
                        
                        
                        
                        
                        
                        
                            <a class="button-square button-social hint--top" data-hint="Github" aria-label="Github" href="https://github.com/davidblitz" rel="me">
                                <i class="fa fa-github-alt" aria-hidden="true"></i>
                            </a>
                        
                        
                        
                        
                    </div>
		
                    <ul class="site-nav">
                        

                    </ul>
                </div>
            </header>

            <div id="container">


<div class="container">
    <article class="post-container" itemscope="" itemtype="http://schema.org/BlogPosting">
        <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Estimating Mutual Information in Bayesian Neural Networks</h1>
    
    <p class="post-date post-line">
        <span>Published <time datetime="2019-02-18" itemprop="datePublished">Mon, Feb 18, 2019</time></span>
        <span>by</span>
        <span itemscope="" itemprop="author" itemtype="https://schema.org/Person">
            <span itemprop="name">
                <a href="https://github.com/davidblitz" itemprop="url" rel="author">David Blitz</a>
            </span>
        </span>
    </p>
    
    
</header>


        <div class="post-content clearfix" itemprop="articleBody">
    

    <p>Let&rsquo;s consider a Bayesian Neural Network, where we consider the hidden layers as random variables, \(Z_i\).
We also consider the input, \(X\), and the learning task, \(Y\), as random variables.
Our goal is to estimate the mutual information between the input and the layers - \(I(X, Z_i)\) -
as well as between the layers and the task - \(I(Z_i, Y)\).
For this, we will make use of the probabilistic forward propagation approximation, as introduced [in the PBP paper].</p>
<h1 id="estimating-ix-z_i">Estimating \(I(X, Z_i)\)</h1>
<p>We have
$$
I(X, Z_i) = H(Z_i) - H(Z_i | X).
$$
We can estimate each of the two entropy terms on the RHS with help of PFP:</p>
<ul>
<li>
<p>\(H(Z_i)\) can just be approximated by taking the entropy of the forward propagated distribution of \(X\). This distribution we approximate in turn by a multivariate gaussian.</p>
</li>
<li>
<p>for the conditional entropy we have:
$$
H(Z_i | X) = \mathbb{E}_x [H(Z_i | x)] = \sum_x p(x) \int p(Z_i | x) \log p(Z_i | x) dx
$$
The PFP approximation results in
$$
p(Z_i | x) \approx \mathcal{N}(Z_i | m^{i}(x), \Sigma^{i}(x) ).
$$
Using the formula for the entropy of a gaussian, we get:
$$
H(Z_i | X) = \sum_x \frac{1}{N} \frac{1}{2} \log(2 \pi e \Sigma^{i}(x)) .
$$
Notice, that this is independent of the mean of the PFP approximation.</p>
</li>
</ul>
<h1 id="estimating-iz_i-y">Estimating \(I(Z_i, Y)\)</h1>
<p>Again, we express the mutual information with entropies:
$$
I(Z_i, Y) = H(Y) - H(Y | Z_i).
$$</p>
<ul>
<li>the first term stays constant over training, so we can just ignore it or estimate it by approximating the distribution of \(Y\) by a gaussian</li>
<li>the conditional entropy is more tricky:
$$
H(Y | Z_i) = \sum_y \int p(y , Z_i) \log p(y | Z_i) dZ_i
$$
$$
= \sum_x \sum_y \int p(y, x, Z_i) \log p(y | Z_i) dZ_i
$$
$$
= \sum_x \sum_y p(x, y) \int p(Z_i | x) \log p(y | Z_i),
$$</li>
</ul>
<p>where we have used the independence of \(Z_i\) and \(Y\).
Now, we can approximate \( p(Z_i | x) \) by a Gaussian via PFP. In order, to integrate, we sampling \(z_i^1, \dots, z_i^S\) from this Gaussian, approximate
\( p(y | z_i^s) \) for each \(1 \leq s \leq S\) via PFP and finally compute
$$
\sum_s \frac{1}{S} \log p(y | z_i^s)
$$.</p>

</div>

        <footer class="post-footer clearfix"><div class="views">
    <span class="views">
    	<style>img {
	  padding-top: 20px;
	}
	</style>
        <img src="https://visitor-badge.glitch.me/badge?page_id=https%3a%2f%2fdavidblitz.github.io%2f2019%2f02%2f18%2festimating-mutual-information-in-bnn%2f" alt="Views" />
    </span>
    </div>
    <div class="share">
    </div>
</footer>


        
    </article>
</div>

            </div>
        </div>

        <footer class="footer">
            <div class="container">
                <div class="site-title-wrapper">
                    <h1 class="site-title">
                        <a href="https://davidblitz.github.io/">BlitzBlog</a>
                    </h1>
                    <a class="button-square button-jump-top js-jump-top" href="#" aria-label="Back to Top">
                        <i class="fa fa-angle-up" aria-hidden="true"></i>
                    </a>
                </div>

                <p class="footer-copyright">
                    <span>&copy; 2022 / Powered by <a href="https://gohugo.io/">Hugo</a></span>
                </p>
                <p class="footer-copyright">
                    <span><a href="https://github.com/roryg/ghostwriter">Ghostwriter theme</a> By <a href="http://jollygoodthemes.com">JollyGoodThemes</a></span>
                    <span>/ <a href="https://github.com/jbub/ghostwriter">Ported</a> to Hugo By <a href="https://github.com/jbub">jbub</a></span>
                </p>
            </div>
        </footer>

        <script src="https://davidblitz.github.io/js/jquery-1.11.3.min.js"></script>
        <script src="https://davidblitz.github.io/js/jquery.fitvids.js"></script>
        <script src="https://davidblitz.github.io/js/scripts.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$'], ['\[','\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    },
    startup: {
      pageReady() {
        return MathJax.startup.defaultPageReady().then(function () {
          var all = window.MathJax.startup.document.getMathItemsWithin(document.body), i;
          for(i = 0; i < all.length; i += 1) {
            console.log(all[i])
            all[i].start.node.parentNode.className += ' has-jax';
          }
        });
      }
    }
  };
  </script>

        
    </body>
</html>

